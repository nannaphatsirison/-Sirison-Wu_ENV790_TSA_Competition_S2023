---
title: "Competition_Forecast_RMD"
author: "Nannaphat Sirison, Lynn Wu"
date: "2023-04-03"
output:
  pdf_document: default
  html_document: default
---

##  INTRODUCTION

```{r}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
#Installing required packages
library(lubridate)
library(ggplot2)
library(forecast)  
library(Kendall)
library(tseries)
library(outliers)
library(tidyverse)
library(smooth)
library(kableExtra)
library(readxl)
library(zoo)
library(openxlsx)
library(writexl)
```
This project aims to utilize time series forecasting tools to model daily load demand. As population increases, and as renewable energy penetration increases, it is important to develop models that help forecast load that can help utilities understand load patterns and long term trends in load. This can help utilities plan for how much generation and capacity they can should be procuring to make sure that load is appropriately met. 

This project aims to find a suitable model that best forecasts daily load. The methodology/ steps that we implemented is as follows:  
> (1) Examine and explore the dataset for any initial trends  
(2) Perform initial analysis of the time series using ACF and PACF plots, and decompositions of time series.  
(3) Train 4 different models to the time series and examine performance against observed data  
(4) Calculate performance metrics for each of the 4 models and chose the best model for a future forecast  
(5) Generated a 5 month future forecast using the best chosen model 

## ABOUT THE DATA

The dataset we are working in provides hourly data from January 1, 2005 to December 31,2010. We have aggregated hourly data into daily data, by taking the average across 24 hours of each day. 

```{r}
#Loading in the data set
df<-read_excel("load.xlsx")
```

```{r}
#DATA WRANGLING
#Taking the average across hours of the day
df$hourly_mean <- rowMeans(df[,3:26])

#Selecting only necessary columns: data and hourly mean
df <- df[, c("date","hourly_mean")]

#Making date column a date format
df$date <- ymd(df$date)

```

```{r}
#Drop NAs
df1 <- df %>% drop_na()

#Converting to time series object (2005.1.1-2010.12.31)
ts <- msts(df1[,2], seasonal.periods = c(7,365.25),start=c(2005,01,01),end=c(2010,12,31))

```

A plot of the dataset we are working with is shown, below with a table of summary statistics. 

```{r, echo= FALSE}
#Initial plot of time series
ts_plot<-ggplot(df1,aes(x=date, y=hourly_mean))+
  geom_line()+
  labs(y="Load", x= "Year", title="Daily Load (averaged across hours)")+
  theme_minimal()
plot(ts_plot)
```

```{r, echo = FALSE}
#Summary of time series
ts_summary<-summary(ts)
ts_summary
```

In the plot above, we see that across the time series we there are obvious fluctuations/ oscillations. These fluctuations are not surprising; it is a common for load to vary based on seasons and time of day, for example, higher load is expected during summer time than spring time. We also observe an overall increasing trend, which is expected as energy intensity increases and population increases.

Based on the historical data set we are using, observed minimum load is at 1525, and maximum load is at 7545. Mean load is 3329 and median load is 3182.

## INITIAL TIME SERIES ANALYSIS

Before running a time series forecast model, we prepare and transform the data to fit the needs of time series forecasting. Before selecting models for time series forecasting, we examine ACF and PACF plots of the original, un-transformed time series (shown below).

```{r, echo=FALSE}
#Original Series: ACF and PACF Plots
par(mfrow=c(1,2))
ACF_Plot <- Acf(ts, lag = 40, plot = TRUE,main="ACF of Hourly Mean")
PACF_Plot <- Pacf(ts, lag = 40,plot = TRUE,main="PACF of Hourly Mean")
```
>Plots not only show strong seasonality but also non-stationarity due to the slow decay.

We then examine the decomposition of the original time series to identify seasonality and trend. Decomposition of the time series is displayed below: 

```{r, echo=FALSE}
#Original Series: Decomposition
ts %>% mstl() %>%
  autoplot()
```
>The trend has a narrow range compared to seasonal components. Both weekly and yearly pattern have strong scales and trend components in daily data.

After performing initial data exploration of aggregated (daily) data, we examine 4 forecasting models. Each model used training data from January 1, 2015 to December 21, 2019, and was compared to test data from January 1st,2020 to December 31st,2020 to check for accuracy. 

##FORECASTING DAILY LOAD

```{r, message=FALSE, warning=FALSE}
#create a subset for training purpose (2005-2009)
n_for = 365
ts_train <- subset(ts,end = length(ts)-n_for)

#create a subset for testing purpose (2009-2010)
ts_test <- subset(ts, start = length(ts)-n_for)

autoplot(ts_train)
autoplot(ts_test)
```

#Model 1: STL + ETS 

Our first forecasting model applies a non-seasonal exponential smoothing model to all seasonally adjusted data.

```{r, echo=TRUE, message=FALSE, warning=FALSE}
#Fit and forecast STL + ETS model to data
ETS_fit <-  stlf(ts_train,h=365)
```

```{r, echo = TRUE}
#Plot model + observed data
autoplot(ts) +
  autolayer(ETS_fit, series="STL + ETS",PI=FALSE) +
  ylab("Daily Load")+
  labs(title= "STL+ETS vs. Observed")
```

We observe that that the STL+ETS model follows the seasonality well (as observed by the coincidental oscillation of the red and black series); however, the magnitude of the forecast does not fit the data we observe well. Forecasted data has a greater magnitude than the observed data when the time series is at its peak but a lower magnitude than observed data when the time series is at its minimum. f. 

```{r}
#WRANGLING FOR KAGGLE SUBMISSION

#Generate 2011 output for Kaggle competition
ETS_fit_kaggle <-  stlf(ts,h=59)
#Save ETS_fit_kaggle as a dataframe
ETS_fit_kaggle_df <- as.data.frame(ETS_fit_kaggle$mean)

#Read in submission template
submission_template<-read_excel("submission_template.xlsx")
#Merge df
submission_template_ETS <- cbind(submission_template,ETS_fit_kaggle_df)
#Remove extra column
submission_template_ETS <- submission_template_ETS[, c("date","x")]
#Rename column from "x" to "load"
colnames(submission_template_ETS) <- c("date","load")

#Save out dataframe as excel file to submit to Kaggle 
write.xlsx(submission_template_ETS, file = "Sirison_Wu_1.xlsx")

```

#Model 2: ARIMA + FOURIER terms

Our second model is called a dynamic harmonic regression model with an ARMA error structure, which adopts a log transformation in the ARIMA model to ensure the forecasts and prediction intervals remain positive. The FOURIER terms determine how quickly the seasonality could change.

```{r ARIMA, echo=TRUE, message=FALSE, warning=FALSE}

ARIMA_Four_fit <- auto.arima(ts_train, 
                             seasonal=FALSE, 
                             lambda=0,
                             xreg=fourier(ts_train, 
                                          K=c(2,48))
                             )

#Forecast with ARIMA fit
ARIMA_Four_for <- forecast(ARIMA_Four_fit,
                           xreg=fourier(ts_train,
                                        K=c(2,48),
                                        h=365),
                           h=365
                           ) 

#Plot foresting results
autoplot(ARIMA_Four_for) + ylab("Hourly Load")

#Plot model + observed data
autoplot(ts) +
  autolayer(ARIMA_Four_for, series="ARIMA_FOURIER",PI=FALSE) +
  ylab("Daily Load")+
  labs(title= "ARIMA+Fourier Terms vs. Observed")
```
We tried different K numbers, such as 4, 6, 12, 24, 36, and 60. We found that although 48 performs the best accuracy compared with the original data, the forecasted still has a greater magnitude than observed data when the time series is at its peak maximum, but has a lower magnitude when the time series is at its minimum.

```{r}
#WRANGLING FOR KAGGLE SUBMISSION

#Generate 2011 output for Kaggle competition
ARIMA_FOURIER_fit_kaggle <-  forecast(ARIMA_Four_for,h=59)
#Save ETS_fit_kaggle as a dataframe
ARIMA_FOURIER_fit_kaggle_df <- as.data.frame(ARIMA_FOURIER_fit_kaggle$mean)

#Merge df
submission_template_ARIMA <- cbind(submission_template,ARIMA_FOURIER_fit_kaggle_df)
#Remove extra column
submission_template_ARIMA <- submission_template_ARIMA[, c("date","x")]
#Rename column from "x" to "load"
colnames(submission_template_ARIMA) <- c("date","load_Model2")

#Merge model1 and model2
combine_model_final<- merge(submission_template_ETS,submission_template_ARIMA,by="date")
colnames(combine_model_final) <- c("date","load_Model1","load_Model2")

#Save out dataframe as excel file to submit to Kaggle 
write.xlsx(combine_model_final, file = "Sirison_Wu_2.xlsx")

```

#Model 3: TBATS

Our third model is the Trigonometric seasonality, Box-Cox transformation (TBATs), which is a model appropriate for handling time series data that has multiple seasonalities. The TBATS approach models seasonal periods using trigonometric based on Fourier series.

```{r, echo=TRUE, message=FALSE, warning=FALSE}
TBATS_fit <- tbats(ts_train)
TBATS_for <- forecast(TBATS_fit, h=365)
```

```{r, echo= TRUE}
#Plot TBATS forecast vs observed data 
autoplot(ts) +
  autolayer(TBATS_for, series="TBATS",PI=FALSE)+
  ylab("Daily Load")+
  labs(title= "TBATS vs. Observed")
```

The TBATS model fits the seasonality well; however the magnitude for both when the time series is at its maximum and its minimum is lower than what observed data. 

```{r}
#WRANGLING FOR KAGGLE SUBMISSION
#Generate 2011 output for Kaggle competition
TBATS_fit_kaggle <- forecast(TBATS_fit, h=59)
#Save TBATS_fit_kaggle as a dataframe
TBATS_fit_kaggle_df <- as.data.frame(TBATS_fit_kaggle$mean)

#Read in submission template
submission_template<-read_excel("submission_template.xlsx")
#Merge df
submission_template_TBATS <- cbind(submission_template,TBATS_fit_kaggle_df)
#Remove extra column
submission_template_TBATS <- submission_template_TBATS[, c("date","x")]
#Rename column from "x" to "load"
colnames(submission_template_TBATS) <- c("date","load")

#Save out dataframe as excel file to submit to Kaggle 
write.xlsx(submission_template_TBATS, file = "Sirison_Wu_3.xlsx")
```

#Model 4. Neural Network Time Series Forecasts

Our fourth model is the Neural Network model. The neural network we used here is a feed forward neural network (FNN), meaning the nodes do not form a cycle or inputs to the FNN produces outputs that do not feed back into the next input. 

```{r}
#NN_fit and forecast
NN_fit <- nnetar(ts_train,p=1,P=0,xreg=fourier(ts_train, K=c(2,12)))
NN_for <- forecast(NN_fit, h=365,xreg=fourier(ts_train, 
                                          K=c(2,12),h=365))
```

```{r, echo = TRUE}
#Plot NN forecast vs observed data 
autoplot(ts) +
  autolayer(NN_for, series="Neutral Network",PI=FALSE)+
  ylab("Daily Load")+
  labs(title= "Neural Network vs. Observed")
```

After trials, we found that NNAR(1,0,12) fits the model best, which means non-seasonal AR is 1, and seasonal AR equals to 0 with 12 hidden nodes.
The neural network model is able to follow both the seasonality and magnitude of the observed data very well. 

```{r,message=FALSE}
#WRANGLING FOR KAGGLE SUBMISSION
#Generate 2011 output for Kaggle competition
NN_fit_kaggle <- forecast(NN_for, h=59)
#Save NN_fit_kaggle as a dataframe
NN_fit_kaggle_df <- as.data.frame(NN_fit_kaggle$mean)

#Read in submission template
submission_template<-read_excel("submission_template.xlsx")
#Merge df
submission_template_NN <- cbind(submission_template,NN_fit_kaggle_df)
#Remove extra column
submission_template_NN <- submission_template_NN[, c("date","x")]
#Rename column from "x" to "load"
colnames(submission_template_NN) <- c("date","load")

#Save out dataframe as excel file to submit to Kaggle 
write.xlsx(submission_template_NN, file = "Sirison_Wu_4.xlsx")
```

##EXAMINING ACCURACY OF FORECASTS 

```{r}
#Model 1: STL + ETS Accuracy
ETS_scores <- accuracy(ETS_fit$mean,ts_test)  

#Model 2: Arima + Fourier Accuracy 
ARIMA_scores <- accuracy(ARIMA_Four_for$mean,ts_test)

#Model 3: TBATS Accuracy 
TBATS_scores <- accuracy(TBATS_for$mean,ts_test)

#Model 4: Neural Network Accuracy 
NN_scores <- accuracy(NN_for$mean,ts_test)
```

#Accuracy Comparison 
```{r, echo= TRUE}
#Graph of all forecast models 
autoplot(ts) +
  autolayer(ETS_fit, series="STL + ETS",PI=FALSE) +
  autolayer(ARIMA_Four_for, series="ARIMA_FOURIER",PI=FALSE) +
  autolayer(TBATS_for, series="TBATS",PI=FALSE)+
  autolayer(NN_for, series="NN",PI=FALSE)+
  ylab("Daily Load")+
  labs(title = "Forecasting Model Comparison")
```
After visually examining how each model performs compares to observed data, it is unclear which model performs the best; therefore, we calculate accuracy metrics for each model.

#Compare performance metrics
```{r}
#create data frame
scores <- as.data.frame(
  rbind(ETS_scores, ARIMA_scores, TBATS_scores, NN_scores)
  )
row.names(scores) <- c("STL+ETS", "ARIMA+Fourier","TBATS","NN")

#choose model with lowest RMSE
best_model_index <- which.min(scores[,"RMSE"])
cat("The best model by RMSE is:", row.names(scores[best_model_index,]))   

#choose model with lowest MAPE
best_model_index <- which.min(scores[,"MAPE"])
cat("The best model by MAPE is:", row.names(scores[best_model_index,])) 
```

We examine 2 metrics for model performance: RMSE (Root Mean Square Error) and MAPE (Mean Absolute Percentage Error). RMSE is a metric that averages the difference between actual and predicted values, meaning it takes errors in magnitude into account. MAPE measures the average percentage difference between actual and predicted values.

```{r echo=FALSE, message=FALSE, warning=FALSE}
#Table of accuracy statistics
kbl(scores, 
      caption = "Forecast Accuracy for Daily Load",
      digits = array(5,ncol(scores))) %>%
  kable_styling(full_width = FALSE, position = "center", latex_options = "hold_position") %>%
  kable_styling(latex_options="striped", stripe_index = which.min(scores[,"RMSE"]))
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
#Table of accuracy statistics
kbl(scores, 
      caption = "Forecast Accuracy for Daily Load",
      digits = array(5,ncol(scores))) %>%
  kable_styling(full_width = FALSE, position = "center", latex_options = "hold_position") %>%
  kable_styling(latex_options="striped", stripe_index = which.min(scores[,"MAPE"]))
```

The best model by RMSE is the TBATS model, while the best model by MAPE is the Neural Network model. This discrepancy is due to the fact that RMSE and MAPE captures different aspects in terms of accuracy. RMSE focuses more on magnitude of errors, while MAPE focuses more on percentage of errors. 

Due to the discrepancy by using just RMSE and MAPE for performance metrics, we also consider other metrics: MAE (mean averaged error) and MPE (mean percentage error). The TBATS model outperforms the Neural Network model in both MAE and MPE. 

##FUTURE FORECAST BASED ON CHOSEN BEST ODEL

In conclusion, after training and testing 4 models: STL+ETS, Arima + Fourier Terms, TBATS, and Neural Network, we conclude that the TBATS model is the best performing model for forecasting daily load.

We then regenerated the TBATS model by training it using the entire dataframe, instead of using a subset, as performed previously. The regenerated TBATS model is then used to forecast daily load 5 months (151 days) into the future. The results of the future forecast using a TBATS model is displayed below: 

```{r, message = FALSE, echo = TRUE}
#Fit new TBATS model that uses the entire dataframe
TBATS_fit_final <- tbats(ts)
#Forecast of new TBATS model 
TBATS_for_final <- forecast(TBATS_fit_final, h=151)
```

```{r, echo = TRUE}
#Plot of future TBATS forecast
autoplot(ts) +
  autolayer(TBATS_for_final, series="TBATS",PI=FALSE)+
  ylab("Daily Load")+
  labs(title= "Observed Data + TBATS Future Forecast")
```

##CONCLUSION

#Discussion and Limitations
We can see that the future forecast 5 months into the future, using the TBATS model looks "smoother" than the rest of the observed data. This is a result of the nature of the model. The TBATS forecast incorporates Box-Cox transformations that reduces variance, and ARMA terms that reduces the impact of random fluctuations. 

We also observe a noticeable jump in load; minimum load seems to increase dramatically compared to the series last minima in the observed data. This dramatic increase may partially be due to the "smoothing" effect of the TBATS modeled as discussed above. Even though we do not expect daily load to change dramatically, we do know that under increasing frequency of extreme weather, unexpected load peaks (outliers) are to be expected. Therefore, even though the TBATS performed best when we evaluated models using accuracy metrics, this may mean that given the nature of our time series data (load data) the TBATS model may not be the model choice that is most applicable since it does not perform as well in capturing outliers. 

Additionally, the unexpected, observed jump in minimum load may also highlight the limitations of time series forecasting.Even though we trained a forecasting model using more than a decade of data, this does not necessarily mean that the future forecast generated will be 100% accurate. Our model operates under the assumption that the same cycles, patterns and trends that persist in the past will persist into the future. 

Overall, this project successfully explored multiple models that are fitted to daily load data. We concluded that the TBATS model outperformed the other 3 models (STL+ETS, Arima + Fourier, and Neural Network) using both a visual comparison and performance metrics (RMSE, MAPE, MSE, and MPE). We were able to generate a future prediction (5 months ahead) for daily load, in which we observed an increase in demand. Nonetheless, we acknowledge the limitations of both the model we chose for future forecasting as well as the limitations of time series forecasting. Regardless, forecasting of future load can still be useful for many stakeholders in the the energy and utility space. Despite the limitations of how accuracy a time series forecast can be, a general idea of what load might look like int he next 10 years can help utilities plan generation and capacity appropriately. 


